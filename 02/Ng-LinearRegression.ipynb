{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvhn07OSYRR7"
   },
   "source": [
    "# Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wU4ZyDHpYRSE"
   },
   "source": [
    "В этой записной книжке рассматривается решение на основе Python для первого упражнения по программированию из курса машинного обучения на Coursera. Пожалуйста, обратитесь к [тексту упражнения] (https://github.com/jdwittenauer/ipython-notebooks/blob/master/exercises/ML/ex1.pdf) для получения подробных описаний и уравнений.\n",
    "\n",
    "В этом упражнении мы реализуем простую линейную регрессию с использованием градиентного спуска и применим ее к модельной задачи. Мы также расширим нашу реализацию на случай нескольких переменных и применим ее к немного более сложному примеру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l04Sx8oCYRSF"
   },
   "source": [
    "## С одной переменной"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Знакомство с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0P-bsvbYRSG"
   },
   "source": [
    "В первой части упражнения перед нами стоит задача реализовать линейную регрессию с одной переменной для прогнозирования прибыли фургона с едой. Предположим, вы являетесь генеральным директором франшизы и рассматриваете возможность открытия новой точки в разных городах. В сети уже есть грузовики в разных городах, и у вас есть данные о прибылях и населении в городах. Моделируя прибыль как линейную функцию от населения, мы ищем прямую, наилучшим образом отражающую эту зависимость.\n",
    "\n",
    "Начнем с импорта некоторых библиотек и изучения данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXi5U3uMYRSH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаем данные, там два столбика. Посмотрим на размер, статистические характеристики, и нарисуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrLqsp90YRSH",
    "outputId": "4cc84fe7-fe6b-4a90-ebad-6e1bc5ec1112"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = '../data/ex1data1.txt'\n",
    "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5flAXu9YRSJ",
    "outputId": "0167e40b-61cb-4f0b-cadd-9bd39c162f95"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO_y0COwYRSL",
    "outputId": "1230364b-0cef-4ef9-c8b4-5ce6bb9a2820"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,6))\n",
    "plt.scatter(data['Population'], data['Profit'])\n",
    "plt.plot([0, data['Population'].max()*1.1], [data['Profit'].mean(), data['Profit'].max()*0.9], 'r--')\n",
    "plt.annotate(\"модель\", (0,data['Profit'].mean()-1), color=\"red\")\n",
    "plt.xlim(left=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель — прямая задается двумя параметрами $\\theta = (\\theta_0, \\theta_1)$, \n",
    "и предсказывает прибыль так:\n",
    "$$\\hat{y}(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "Ошибка модели для заданного $x_i$ (населения) есть разность между предсказанной и реальной прибылью: $\\hat{y}(x_i)-y_i = \\theta_0 + \\theta_1 x_i - y_i$. Определим *функцию потерь* как половину среднего квадрата ошибки:\n",
    "$$ MSE(\\theta) = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left(\\hat{y}(x_i)-y_i\\right)^2 = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left( \\theta_0 + \\theta_1 x_i - y_i \\right)^2.$$\n",
    "\n",
    "Если данных много, то функция, определенная как питоновский цикл от 1 до n, будет работать медленно. Надо использовать векторные операции NumPy. Будем прицеливаться на общий случай, когда прибыль зависит от m параметров $x^{1}, x^{2}, \\ldots, x^{m}$, модель выглядит как $\\hat{y} = \\theta_0 + \\theta_1 x^1 + \\theta_2 x^2 +\\ldots + \\theta_m x^m$, поэтому вычисление $\\hat{y}$ тоже надо делать векторно, как $\\theta \\cdot x$. Но надо как-то включить $\\theta_0$.\n",
    "\n",
    "$$\\hat{y} = \\theta_0\\cdot 1 + \\theta_1 \\cdot x^1 + \\theta_2 \\cdot x^2 + \\ldots +\\theta_m \\cdot x^m$$\n",
    "\n",
    "Мы сделаем это, добавляя фиктивный столбик единиц в data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.insert(0, 'Ones', np.ones(len(data)))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим все столбики, кроме $y$, в матрицу, и умножим ее на вектор параметров $\\theta$\n",
    "\n",
    "$$\\hat{y} = X \\left[ \\begin{array}{c}\\theta_0 \\\\ \\theta_1\\end{array} \\right] =\n",
    "\\begin{array}{|cc|}\n",
    " \\hline\n",
    " 1 & 6.1\\cr\n",
    " 1 & 5.5\\cr\n",
    " ...\\cr\n",
    " 1 & 5.4\\cr\n",
    " \\hline\n",
    "\\end{array}\n",
    "\\cdot\n",
    "\\begin{array}{|c|}\n",
    " \\hline\n",
    " \\theta_0 \\cr\n",
    " \\theta_1 \\cr\n",
    " \\hline\n",
    "\\end{array} = \n",
    "\\begin{array}{|l|}\n",
    "\\hline\n",
    " 1\\cdot \\theta_0 + x_1 \\cdot \\theta_1\\cr\n",
    " 1\\cdot \\theta_0 + x_2 \\cdot \\theta_1\\cr\n",
    " ...\\cr\n",
    " 1\\cdot \\theta_0 + x_{n}\\cdot \\theta_1\\cr\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "Получим матрицу ошибок. Остается вычесть столбик из $y_i$ (`Profit` в датафрейме), возвести полученную разность поэлементно в квадрат, и вычислить среднее\n",
    "$$MSE(\\theta) = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left( \\theta_0 + \\theta_1 x_i - y_i \\right)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим X (обучающую выборку) и y (целевой вектор)\n",
    "X = data.iloc[:,:-1].to_numpy()\n",
    "y = data['Profit'].to_numpy().reshape((-1,1))\n",
    "\n",
    "def MSE(theta):\n",
    "    return np.mean((X.dot(np.array(theta).reshape(2,1)) - y)**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим совпадение вычисления в питоновском цикле с векторным numpy\n",
    "theta = [1, 2]\n",
    "pythonSE = 0\n",
    "for i in range(len(data)):\n",
    "    pythonSE +=(theta[0] + theta[1]*data['Population'][i] - data['Profit'][i])**2\n",
    "pythonMSE = pythonSE/(2*len(data))\n",
    "pythonMSE, MSE(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск — случай одной переменной c одним параметром\n",
    "\n",
    "В качестве разминки рассмотрим упрощенную задачу с одним параметром. Для этого будем искать наилучшее приближение только среди прямых, проходящие через начало координат, то есть в качестве вектора параметров $\\theta$ брать только векторы с $\\theta_0=0$. В этом случае формула для функции потерь превращается в \n",
    "$$MSE_1(\\theta_1) = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left( \\theta_1 x_i - y_i \\right)^2.$$\n",
    "Нарисуем, как меняется эта функция при изменении нашего единственного параметра $\\theta \\in (0, 5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE1(th1):\n",
    "    return MSE([0, th1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "MAX = 5\n",
    "\n",
    "\n",
    "def setupAxes(left, right):\n",
    "    # настройка отображения осей\n",
    "    left.set_xlim(-2, MAX)\n",
    "    left.set_ylim(0, mse[-1])\n",
    "    left.set_xlabel(r'$\\theta_1$')\n",
    "    left.set_ylabel(r'$MSE(\\theta_1)$')\n",
    "    left.grid()\n",
    "    \n",
    "    right.set_xlim(x[0], x[-1])\n",
    "    right.set_ylim(-5, 1.1 * data['Profit'].max())\n",
    "    right.set_xlabel('x')\n",
    "    right.set_ylabel('y')\n",
    "    right.grid()\n",
    "\n",
    "fig, (left, right) = plt.subplots(1,2, figsize=(9,5))\n",
    "\n",
    "# левый график - зависимость функции потерь от выбора параметра\n",
    "theta1 = np.linspace(-2, MAX, 300)                    # диапазон значений параметра theta1\n",
    "mse = [MSE1(th1) for th1 in theta1]                   # значения функции потерь при этих параметрах \n",
    "plot_thetaMSE, = left.plot(theta1, mse)               # график MSE(theta1)\n",
    "cur_loss = left.scatter(theta1[0], mse[0], color='r')\n",
    "\n",
    "\n",
    "# правый график - данные и текущее приближение модели\n",
    "x = np.linspace(0, 1.1*data['Population'].max(), 300) # диапазон значения населения\n",
    "scatter_data = right.scatter(data['Population'], data['Profit']) #  точки обучающей выборки\n",
    "cur_model, = right.plot(x, [0]*len(x), 'r--')\n",
    "\n",
    "setupAxes(left, right)\n",
    "\n",
    "def animate(i):\n",
    "    cur_loss.set_offsets((theta1[i], mse[i])) # рисуем текущую точку \"параметр - потери\"\n",
    "    cur_model.set_data(x, theta1[i]*x)        # рисуем график модели, соответствующей параметру\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(theta1), interval=10, repeat=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем искать оптимум итерационно, получая последовательность $\\theta_1^i$. Заметим, что градиент для функции одной переменной - это просто производная. Рассмотрим правило получения последовательных приближений в виде\n",
    "$$\\theta_1^{i+1} = \\theta_1^i - \\alpha MSE_1'(\\theta_1^i), $$\n",
    "где $\\alpha$ — некоторое положительное число, называемое скоростью обучения. Заметим, что если функция возрастает при движении вправо, то ее производная положительна, а значит, из-за добавки $-\\alpha MSE'$ следующее значение параметра будет левее текущего.\n",
    "\n",
    "Наоборот, если при текущем значении параметра $\\theta^i_1$ функция убывает при движении вправо, то ее производная отрицательна, а значит, из-за добавки $-\\alpha MSE'$ следующее значение параметра будет правее текущего.\n",
    "\n",
    "Таким образом, при небольших значениях $\\alpha$ мы получим движение в правильном направлении — в сторону минимума функции потерь.\n",
    "\n",
    "Найдем производную функции потерь по $\\theta_1$:\n",
    "$$MSE_1'(\\theta_1) = \\left(\\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left( \\theta_1 x_i - y_i \\right)^2\\right)' = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left(\\left( \\theta_1 x_i - y_i \\right)^2\\right)' = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}2(\\theta_1 x_i - y_i)\\cdot\\left( \\theta_1 x_i - y_i \\right)' = \\displaystyle\\frac{1}{n}\\sum^{n}_{i=1}(\\theta_1 x_i - y_i)x_i$$\n",
    "То есть производная половины среднеквадратичной ошибки — это среднее арифметическое ошибок, умноженных на значения признаков.\n",
    "\n",
    "Реализуем это в виде функции и построим итерационно до 20 точек (либо до нахождения параметра, дающего производную меньше $10^{-4}$) по формуле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим X (обучающую выборку) и y (целевой вектор)\n",
    "cols = data.shape[1]\n",
    "X = data.iloc[:,0:cols-1].to_numpy()\n",
    "y = data.iloc[:,cols-1:cols].to_numpy().reshape((-1,1))\n",
    "\n",
    "def MSE(X, y, theta):\n",
    "    return np.mean((X.dot(theta.reshape(2,1)) - y)**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "TH1 = [4.5]\n",
    "th1_old = TH1[-1]\n",
    "dmse = dMSE1(th1_old, data)\n",
    "while abs(dmse)>=0.0001 and len(TH1)<20:\n",
    "    dmse = dMSE1(th1_old, data)\n",
    "    th1_new = th1_old - ALPHA * dmse\n",
    "    TH1.append(th1_new)\n",
    "    th1_old = th1_new\n",
    "TH1, dmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполните клетку ниже ▼.\n",
    "\n",
    "Как видно, процесс расходится. Это вызвано тем, что значения $\\theta_1$ и $MSE_1(\\theta_1)$ различаются на два порядка. Поэтому производная очень велика и при выбранной скорости обучения $\\alpha$, на следующей итерации мы не просто не сдвигаемся ближе к минимуму, а \"перелетаем\" через него в точку, которая еще дальше от него, чем текущая. \n",
    "\n",
    "**ЗАДАНИЯ:**  \n",
    "\n",
    "1. Подберите скорость обучения `ALPHA` в клетке выше ▲ так, чтобы появилась сходимость; \n",
    "2. добавьте еще одну клетку, где покажите значение найденного оптимального параметра;\n",
    "3. перезапустите анимацию в клетке ниже ▼. Сохраните тетрадку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "MAX = 5\n",
    "\n",
    "fig, (left, right) = plt.subplots(1,2, figsize=(9,5))\n",
    "\n",
    "# левый график - зависимость функции потерь от выбора параметра и приближения\n",
    "theta1 = np.linspace(-2, MAX, 300)                        # диапазон значений параметра\n",
    "mse = [MSE1(th1, data) for th1 in theta1]                # значения функции потерь при этих параметрах \n",
    "left.set_xlim(-2, MAX)\n",
    "left.set_ylim(0, mse[-1])\n",
    "left.set_xlabel(r'$\\theta_1$')\n",
    "left.set_ylabel(r'$MSE(\\theta_1)$')\n",
    "plot_thetaMSE, = left.plot(theta1, mse)\n",
    "\n",
    "mse1s = [MSE1(th1, data) for th1 in TH1]\n",
    "points = np.array(list(zip(TH1, mse1s)))\n",
    "cur_loss = left.scatter(*points[0], color='r', s=10)\n",
    "\n",
    "# правый график - данные и текущее приближение модели\n",
    "x = np.linspace(0, 1.1*data['Population'].max(), 300) # значения населения, на которых рисуем второй график\n",
    "right.set_xlim(x[0], x[-1])\n",
    "right.set_ylim(-5, 1.1*data['Profit'].max())\n",
    "right.set_xlabel('x')\n",
    "right.set_ylabel('y')\n",
    "right.grid()\n",
    "scatter_data = right.scatter(data['Population'], data['Profit'])\n",
    "cur_model, = right.plot(x, [0]*len(x), 'r--')\n",
    "\n",
    "def animate(i):\n",
    "    cur_loss.set_offsets(points[:i]) # рисуем приближения \"параметр_i - потери_i\"\n",
    "    cur_model.set_data(x, TH1[i]*x)        # рисуем график модели, соответствующей параметру\n",
    "    \n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(TH1), interval=500, repeat=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск — случай одной переменной c двумя параметрами\n",
    "\n",
    "Вернемся к случаю, когда прямая может сдвигаться от начала координат, то есть в приближении \n",
    "\n",
    "$$\\hat{y}(x) = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "параметр $\\theta_0$ не равен 0. Теперь функция потерь зависит от двух переменных и вместо производной у нее будет градиент. \n",
    "\n",
    "$$MSE(\\theta) = \\displaystyle\\frac{1}{2n}\\sum^{n}_{i=1}\\left( \\theta_0 + \\theta_1 x_i - y_i \\right)^2.$$\n",
    "\n",
    "$$\\mathop{\\mathrm{grad}} MSE = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "\\displaystyle\\frac{\\partial MSE(\\theta)}{\\partial \\theta_0} \\\\ \n",
    "\\displaystyle\\frac{\\partial MSE(\\theta)}{\\partial \\theta_1} \n",
    "\\end{array}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "\\displaystyle\\frac{1}{n}\\sum^{n}_{i=1}\\left( \\theta_0 + \\theta_1 x_i - y_i \\right) \\\\ \n",
    "\\displaystyle\\frac{1}{n}\\sum^{n}_{i=1}\\left( \\theta_0 + \\theta_1 x_i - y_i \\right)x_i \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Давайте нарисуем ее в виде контурного графика и нанесем векторы градиентов в некоторых точках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 401, 351\n",
    "th0 = np.linspace(-40, 40, N)\n",
    "th1 = np.linspace(-2, 5, M)\n",
    "TH0, TH1 = np.meshgrid(th0, th1)\n",
    "\n",
    "th0m = TH0.reshape(M, N, 1)\n",
    "th1m = TH1.reshape(M, N, 1)\n",
    "Xm = data['Population'].to_numpy().reshape(1,1,-1)\n",
    "ym = data['Profit'].to_numpy().reshape(1,1,-1)\n",
    "\n",
    "zMSE = ((th0m + th1m*Xm - ym)**2).mean(axis=2) / 2\n",
    "\n",
    "t0 = np.linspace(-40, 40, 9)\n",
    "t1 = np.linspace(-2, 5, 8)\n",
    "T0, T1 = np.meshgrid(t0, t1)\n",
    "t0m = T0.reshape(8, 9, 1)\n",
    "t1m = T1.reshape(8, 9, 1)\n",
    "dMSEdtg0 =  (t0m + t1m*Xm - ym).mean(axis=2) \n",
    "dMSEdtg1 = ((t0m + t1m*Xm - ym)*Xm).mean(axis=2) \n",
    "\n",
    "G = np.gradient(zMSE)\n",
    "dMSEdt0 = G[0][::50,::50]\n",
    "dMSEdt1 = G[1][::50,::50]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(r'$\\theta_0$')\n",
    "ax.set_ylabel(r'$\\theta_1$')\n",
    "ax.set_title(r'$MSE(\\theta)$')\n",
    "ax.grid()\n",
    "ax.contour(TH0,TH1,zMSE, levels=50)\n",
    "ax.quiver(TH0[::50,::50], TH1[::50,::50], dMSEdt0, dMSEdt1)\n",
    "ax.quiver(T0, T1, dMSEdtg0, dMSEdtg1, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th0.shape, th1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th0[200], th1[150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((1.5*data['Population']-data['Profit'])**2).mean()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH0[150,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zMSE[150,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом случае итерационная формула вместо \n",
    "\n",
    "$$\\theta_1^{i+1} = \\theta_1^i - \\alpha MSE_1'(\\theta_1^i) $$\n",
    "\n",
    "принимает вид\n",
    "\n",
    "$$\\left[\\begin{array}{c} \\theta_0^{i+1} \\\\ \\theta_1^{i+1}\\end{array}\\right]= \\left[\\begin{array}{c} \\theta_0^{i} \\\\ \\theta_1^{i}\\end{array}\\right] - \\alpha \\mathop{\\mathrm{grad}} MSE(\\theta^i) = \n",
    "\\left[\\begin{array}{c} \\theta_0^{i} \\\\ \\theta_1^{i}\\end{array}\\right] - \\alpha \\left[\\begin{array}{c} \\frac{\\partial MSE(\\theta^{i})}{\\partial \\theta_0} \\\\ \\frac{\\partial MSE(\\theta^{i})}{\\partial \\theta_0} \\end{array}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlcDUYgDYRSL"
   },
   "source": [
    "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8Ewi5xdYRSM"
   },
   "source": [
    "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1tTbbjfYRSN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALJJvFLxYRSO"
   },
   "source": [
    "The cost function is expecting numpy matrices so we need to convert X and y before we can use them.  We also need to initialize theta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONRhjSHNYRSP"
   },
   "outputs": [],
   "source": [
    "X = np.matrix(X.values)\n",
    "y = np.matrix(y.values)\n",
    "theta = np.matrix(np.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjdUTgIdYRSP"
   },
   "source": [
    "Here's what theta looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPGeejWoYRSP",
    "outputId": "de0f3ced-cb0d-410e-ef66-6fe158551c6f"
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVeCU74FYRSP"
   },
   "source": [
    "Let's take a quick look at the shape of our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSRoHlsTYRSP",
    "outputId": "b7a0b48c-a1c4-4d88-82c3-f8a4df1db1ef"
   },
   "outputs": [],
   "source": [
    "X.shape, theta.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kFH42zoYRSQ"
   },
   "source": [
    "Now let's compute the cost for our initial solution (0 values for theta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb2gT7UaYRSQ",
    "outputId": "945228fe-2823-4ad5-884e-7712f320c502"
   },
   "outputs": [],
   "source": [
    "computeCost(X, y, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz-gOf--YRSQ"
   },
   "source": [
    "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules defined in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx_Sk9tiYRSQ"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(X, y, theta, alpha, iters):\n",
    "    temp = np.matrix(np.zeros(theta.shape))\n",
    "    parameters = int(theta.ravel().shape[1])\n",
    "    cost = np.zeros(iters)\n",
    "    \n",
    "    for i in range(iters):\n",
    "        error = (X * theta.T) - y\n",
    "        \n",
    "        for j in range(parameters):\n",
    "            term = np.multiply(error, X[:,j])\n",
    "            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))\n",
    "            \n",
    "        theta = temp\n",
    "        cost[i] = computeCost(X, y, theta)\n",
    "        \n",
    "    return theta, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYZw84kZYRSR"
   },
   "source": [
    "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM4K9O_bYRSR"
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N40OX1xgYRSR"
   },
   "source": [
    "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uda_3cx-YRSR",
    "outputId": "19c0bd8b-6a74-4cc4-e997-8a04540e66c3"
   },
   "outputs": [],
   "source": [
    "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc2BY-rJYRSS"
   },
   "source": [
    "Finally we can compute the cost (error) of the trained model using our fitted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAIqk53zYRSS",
    "outputId": "b570417f-ccac-4509-d0d6-bb8bca2ba2a9"
   },
   "outputs": [],
   "source": [
    "computeCost(X, y, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0X70dMCYRSS"
   },
   "source": [
    "Now let's plot the linear model along with the data to visually see how well it fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rQBl49tYRSS",
    "outputId": "36ee9a00-22e9-475f-b4e2-708cab572b6e"
   },
   "outputs": [],
   "source": [
    "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
    "f = g[0, 0] + (g[0, 1] * x)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh9FaYkvYRST"
   },
   "source": [
    "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5esvR5iYRST",
    "outputId": "7eab7dc5-65f1-4a25-99d4-bca4496bb83b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMn2nX2rYRST"
   },
   "source": [
    "## Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hKbmh2pYRSU"
   },
   "source": [
    "Exercise 1 also included a housing price data set with 2 variables (size of the house in square feet and number of bedrooms) and a target (price of the house).  Let's use the techniques we already applied to analyze that data set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cHsgSS4YRSU",
    "outputId": "772cd882-427d-4928-a7ba-fe6c93ebf0a5"
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '\\data\\ex1data2.txt'\n",
    "data2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KowucinpYRSU"
   },
   "source": [
    "For this task we add another pre-processing step - normalizing the features.  This is very easy with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyVWQloRYRSU",
    "outputId": "53ee0d01-2bd4-48ee-e8ca-4607b77bbd29"
   },
   "outputs": [],
   "source": [
    "data2 = (data2 - data2.mean()) / data2.std()\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8--6Y-XkYRSV"
   },
   "source": [
    "Now let's repeat our pre-processing steps from part 1 and run the linear regression procedure on the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4QHnvsFYRSV",
    "outputId": "a0677456-1861-4b4f-c87d-38455f591ce6"
   },
   "outputs": [],
   "source": [
    "# add ones column\n",
    "data2.insert(0, 'Ones', 1)\n",
    "\n",
    "# set X (training data) and y (target variable)\n",
    "cols = data2.shape[1]\n",
    "X2 = data2.iloc[:,0:cols-1]\n",
    "y2 = data2.iloc[:,cols-1:cols]\n",
    "\n",
    "# convert to matrices and initialize theta\n",
    "X2 = np.matrix(X2.values)\n",
    "y2 = np.matrix(y2.values)\n",
    "theta2 = np.matrix(np.array([0,0,0]))\n",
    "\n",
    "# perform linear regression on the data set\n",
    "g2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)\n",
    "\n",
    "# get the cost (error) of the model\n",
    "computeCost(X2, y2, g2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c-b5Ek4YRSV"
   },
   "source": [
    "We can take a quick look at the training progess for this one as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDSyXJpfYRSV",
    "outputId": "be4aa519-1d86-48f3-af52-10322aef25ef"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(np.arange(iters), cost2, 'r')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost')\n",
    "ax.set_title('Error vs. Training Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Xb6XGS-YRSW"
   },
   "source": [
    "Instead of implementing these algorithms from scratch, we could also use scikit-learn's linear regression function.  Let's apply scikit-learn's linear regressio algorithm to the data from part 1 and see what it comes up with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPEyWlj7YRSY",
    "outputId": "6908f208-3395-481a-d3b5-48fe1a9be34f"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vvUvULBYRSY"
   },
   "source": [
    "Here's what the scikit-learn model's predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-k076loYRSY",
    "outputId": "154be3c2-23cb-43b5-f510-271a27e86f07"
   },
   "outputs": [],
   "source": [
    "x = np.array(X[:, 1].A1)\n",
    "f = model.predict(X).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.plot(x, f, 'r', label='Prediction')\n",
    "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
    "ax.legend(loc=2)\n",
    "ax.set_xlabel('Population')\n",
    "ax.set_ylabel('Profit')\n",
    "ax.set_title('Predicted Profit vs. Population Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NU952GjYRSZ"
   },
   "source": [
    "That's it!  Thanks for reading.  In Exercise 2 we'll take a look at logistic regression for classification problems."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Копия блокнота \"Копия блокнота \"ML-Exercise1.ipynb\"\"",
   "provenance": [
    {
     "file_id": "1OPWRtUPan7ZnGnrhX-VZ22tjeru4kjj5",
     "timestamp": 1614746619819
    },
    {
     "file_id": "https://github.com/jdwittenauer/ipython-notebooks/blob/master/notebooks/ml/ML-Exercise1.ipynb",
     "timestamp": 1614746584907
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
